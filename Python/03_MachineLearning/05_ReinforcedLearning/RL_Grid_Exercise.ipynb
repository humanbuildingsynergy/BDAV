{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal path: [(0, 0), (1, 0), (2, 0), (2, 1), (2, 2), (3, 2), (3, 3), (3, 4), (4, 4)]\n",
      "O     X   \n",
      "O X       \n",
      "O O O X   \n",
      "X   O O O \n",
      "    X   O \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the grid\n",
    "grid = [\n",
    "    ['S', ' ', ' ', 'X', ' '],\n",
    "    [' ', 'X', ' ', ' ', ' '],\n",
    "    [' ', ' ', ' ', 'X', ' '],\n",
    "    ['X', ' ', ' ', ' ', ' '],\n",
    "    [' ', ' ', 'X', ' ', 'G']\n",
    "]\n",
    "\n",
    "# Define actions\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "# Q-table: 3D array to store Q-values for each state-action pair\n",
    "# Dimensions: 5 (rows) x 5 (columns) x 4 (actions)\n",
    "q_table = np.zeros((5, 5, 4))\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "# Learning rate (alpha): Determines how much new information overrides old information\n",
    "# Range: 0.0 to 1.0\n",
    "# - Low value: Slower learning, but more stable\n",
    "# - High value: Faster learning, but may oscillate or fail to converge\n",
    "# 0.1 is a common starting point, balancing learning speed and stability\n",
    "alpha = 0.1\n",
    "\n",
    "# Discount factor (gamma): Determines the importance of future rewards\n",
    "# Range: 0.0 to 1.0\n",
    "# - Low value: Agent focuses on immediate rewards\n",
    "# - High value: Agent considers long-term rewards more strongly\n",
    "# 0.9 is often used, encouraging the agent to consider long-term consequences while still prioritizing nearer rewards\n",
    "gamma = 0.9\n",
    "\n",
    "# Exploration rate (epsilon): Controls the trade-off between exploration and exploitation\n",
    "# Range: 0.0 to 1.0\n",
    "# - Low value: Agent exploits known information more (greedy behavior)\n",
    "# - High value: Agent explores new actions more frequently\n",
    "# 0.1 means the agent will explore random actions 10% of the time, and choose the best known action 90% of the time\n",
    "# This value is often decreased over time to reduce exploration as the agent learns\n",
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "# Function to get next state\n",
    "def get_next_state(state, action):\n",
    "    \"\"\"\n",
    "    Determines the next state given the current state and action.\n",
    "    \n",
    "    This function implements the transition model of the environment, \n",
    "    defining how the agent moves in the grid world.\n",
    "    \n",
    "    Parameters:\n",
    "    - state: A tuple (x, y) representing the current position in the grid\n",
    "    - action: A string representing the chosen action ('up', 'down', 'left', 'right')\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple (x, y) representing the next state\n",
    "    \n",
    "    Note:\n",
    "    - The function checks for grid boundaries and obstacles ('X')\n",
    "    - If the move is invalid (hits a wall or obstacle), the agent stays in the current state\n",
    "    \"\"\"\n",
    "    x, y = state\n",
    "    if action == 'up' and x > 0 and grid[x-1][y] != 'X':\n",
    "        return (x-1, y)\n",
    "    elif action == 'down' and x < 4 and grid[x+1][y] != 'X':\n",
    "        return (x+1, y)\n",
    "    elif action == 'left' and y > 0 and grid[x][y-1] != 'X':\n",
    "        return (x, y-1)\n",
    "    elif action == 'right' and y < 4 and grid[x][y+1] != 'X':\n",
    "        return (x, y+1)\n",
    "    return state\n",
    "\n",
    "# Function to get reward\n",
    "def get_reward(state):\n",
    "    \"\"\"\n",
    "    Determines the reward for the given state.\n",
    "    \n",
    "    This function implements the reward model of the environment,\n",
    "    defining the immediate feedback the agent receives for being in a particular state.\n",
    "    \n",
    "    Parameters:\n",
    "    - state: A tuple (x, y) representing the position in the grid\n",
    "    \n",
    "    Returns:\n",
    "    - A numerical value representing the reward\n",
    "    \n",
    "    Reward structure:\n",
    "    - Reaching the goal (G): +10 (highest reward to encourage finding the goal)\n",
    "    - Hitting an obstacle (X): -1 (penalty to discourage running into obstacles)\n",
    "    - Any other move: -0.1 (small penalty to encourage finding the goal quickly)\n",
    "    \n",
    "    Note: This reward structure encourages the agent to find the goal\n",
    "    while avoiding obstacles and minimizing the number of steps.\n",
    "    \"\"\"\n",
    "    x, y = state\n",
    "    if grid[x][y] == 'G':\n",
    "        return 10\n",
    "    elif grid[x][y] == 'X':\n",
    "        return -1\n",
    "    else:\n",
    "        return -0.1\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 1000\n",
    "\n",
    "# Outer loop: Repeat the learning process for a number of episodes\n",
    "for episode in range(num_episodes):\n",
    "    # Initialize the starting state for each episode\n",
    "    state = (0, 0)  # Start state (top-left corner)\n",
    "    \n",
    "    # Inner loop: Continue taking actions until the goal state is reached\n",
    "    while grid[state[0]][state[1]] != 'G':\n",
    "        # Choose action using epsilon-greedy strategy\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            # Exploration: Choose a random action\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            # Exploitation: Choose the action with the highest Q-value\n",
    "            action = actions[np.argmax(q_table[state[0]][state[1]])]\n",
    "        \n",
    "        # Take the chosen action and observe the result\n",
    "        next_state = get_next_state(state, action)\n",
    "        reward = get_reward(next_state)\n",
    "        \n",
    "        # Update Q-table using the Q-learning formula\n",
    "        # Q(s,a) = (1 - α) * Q(s,a) + α * (r + γ * max(Q(s',a')))\n",
    "        current_q = q_table[state[0]][state[1]][actions.index(action)]\n",
    "        max_future_q = np.max(q_table[next_state[0]][next_state[1]])\n",
    "        new_q = (1 - alpha) * current_q + alpha * (reward + gamma * max_future_q)\n",
    "        q_table[state[0]][state[1]][actions.index(action)] = new_q\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "# Function to find optimal path\n",
    "def find_optimal_path():\n",
    "    state = (0, 0)\n",
    "    path = [state]\n",
    "    \n",
    "    while grid[state[0]][state[1]] != 'G':\n",
    "        action = actions[np.argmax(q_table[state[0]][state[1]])]\n",
    "        state = get_next_state(state, action)\n",
    "        path.append(state)\n",
    "    \n",
    "    return path\n",
    "\n",
    "# Print optimal path\n",
    "optimal_path = find_optimal_path()\n",
    "print(\"Optimal path:\", optimal_path)\n",
    "\n",
    "# Visualize the path\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if (i, j) in optimal_path:\n",
    "            print('O', end=' ')\n",
    "        else:\n",
    "            print(grid[i][j], end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
